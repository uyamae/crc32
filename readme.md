# CRC32 の計算とC++ での実装

CRC32 の本来の計算手順を、テーブル利用による最適化コードに変換するときの考え方の自分用メモです。

* CRC32 の原理的な説明は省きます。なんとなく分かっている感じ。
* python のcrc32 と同じ結果になれば正しい実装と判断しています。

## CRC32 の計算の基本

入力データをビット列ととらえてmodulo 2 の計算を適用する

### 多項式

* CRC32　の計算で使用するのは```x^32 + x^26 + x^23 + x^22 + x^16 + x^12 + x^11 + x^10 + x^8 + x^7 + x^5 + x^4 + x^2 + x^1 + 1```
* 各項の乗数がビットの位置に対応し、0x100000100110000010001110110110111 という33 bit のデータになる
  * 16 進数表記だと0x104c11db7

### modulo 2 の計算

多項式を除数、入力データのビット列を被除数として以下の手順を繰り返す。

1. 被除数と除数のMSB が一致するよう除数をシフトする
2. 被除数全体とシフトした除数のxor を計算した結果が新たな被除数となる
    * もとの被除数のMSB はxor で必ず0 になる
3. 1, 2 を繰り返して被除数のMSB の位置が除数のMSB より小さくなったら終了

## C++, リトルエンディアン環境での計算

入力データが'a' の文字の場合

* 'a' = 0x61 = 01100001
* ビット列で考えると先頭は最下位ビットとなるため、ビット順序を反転して 10000110

実装上、実用上の都合で

* 入力データのすべてのビットにxor を適用するため、末尾に32bit の0 を付加する
    * 10000110 => 1000011000000000000000000000000000000000
* ビット列の先頭に0 が32bit 以上連続している場合、0 の数が変わってもmodulo2 の結果が同じになるのでそれを防ぐために入力データの先頭32 bit を反転する
    * 1000011000000000000000000000000000000000 => 0111100111111111111111111111111100000000
    * 最終結果の32 bit は再度反転する

### 計算の例

```
入力データ       :10000110
末尾に0 を付加   :1000011000000000000000000000000000000000
先頭32 bit を反転:0111100111111111111111111111111100000000
除数をシフト     : 100000100110000010001110110110111
xor             :0011100011001111101110001001001011000000
除数をシフト     :  100000100110000010001110110110111
xor             :0001100001010111100110110010010000100000
除数をシフト     :   100000100110000010001110110110111
xor             :0000100000011011100010101111111101010000
除数をシフト     :    100000100110000010001110110110111
xor             :0000000000111101100000100001001011101000
除数をシフト     :          100000100110000010001110110110111
被除数が32 bit 以下になったので最後のxor が結果となる
xor             :0000000000111101100000100001001011101000
結果            :--------00111101100000100001001011101000
ビットを反転     :--------11000010011111011110110100010111
結果をリトルエンディアンの32 bit 値とするとC++ の数値と見る場合ビット順序を反転する
11000010011111011110110100010111 => 11101000101101111011111001000011 => 0xe8b7be43
```

python での計算結果と一致する

```python
>>> hex(binascii.crc32(b"a"))
'0xe8b7be43'
```

## C++ での実装

* 入力は1 byte の型(uint8_t など) の配列
* 多項式はx^32 が先頭ビットとなり、リトルエンディアンでは最下位ビットとなる
    * ビット順序を反転する
    * x^32 に対応するビットはxor で必ず0 になり捨てるだけなので省略する
    * 0x104c11db7 =[先頭ビットを省略]=> 0x04c11db7 =[ビット順序を反転]=> 0x3db88320
* 先頭8 bit にmodulo 2 を適用すると、その部分の計算結果は0 となり、捨ててしまってもよい
* 後続の32 bit には先頭8 bit の内容に応じて多項式の下位32 bit 部分をシフトしたものがxor される
* ビットごとのxor は計算順序が変わっても結果は同じになり、後続32 bit にxor される値は先頭8 bit の値によって決まるので、後続32 bit の内容にかかわらず多項式のxor を先に計算しておくことができる

### 多項式のxor の事前計算

入力が'a' の場合の例より

```
先頭32 bit を反転:0111100111111111111111111111111100000000
除数をシフト     : 100000100110000010001110110110111
xor             :0011100011001111101110001001001011000000
除数をシフト     :  100000100110000010001110110110111
xor             :0001100001010111100110110010010000100000
除数をシフト     :   100000100110000010001110110110111
xor             :0000100000011011100010101111111101010000
除数をシフト     :    100000100110000010001110110110111
xor             :0000000000111101100000100001001011101000
↓の値は使用しない
除数をシフト     :          100000100110000010001110110110111
```

除数を抜き出し、後続32 bit にかかる部分をxor

```
除数をシフト     : 100000100110000010001110110110111
除数をシフト     :  100000100110000010001110110110111
除数をｘｏｒ     :--------10101000011001001101101100100000
除数をシフト     :   100000100110000010001110110110111
除数をｘｏｒ     :--------11100100011101010000000001010000
除数をシフト     :    100000100110000010001110110110111
除数をｘｏｒ     :--------11000010011111011110110111101000
```

入力データと、先にxor した多項式を計算

```
先頭32 bit を反転:0111100111111111111111111111111100000000
除数ｘｏｒ       :--------11000010011111011110110111101000
xor             :0000000000111101100000100001001011101000
順に計算した結果 :--------00111101100000100001001011101000
```

結果が一致しているため、除数の事前計算でも問題ない。

### 計算結果のテーブル化

データのビット列の次に処理する8 bit の値から後続32 bit にxor すべき値は決定できるためこれをテーブルとして用意しておく。上の例の先頭8 bit の01111001 をC++ で表現するとビット順序を反転して10011110 = 0x9e = 158 となり、事前計算の結果は同様に11000010011111011110110111101000 のビット順序を反転すると00010111101101111011111001000011 = 0x17b7be43 となるので、テーブル（配列）の158 番目の値が0x17b7be43 ということになる。

## C++ でのテーブルを利用したCRC32 計算の実装

* 入力データ"abcd" =[16進数]=> 0x61 0x62 0x63 0x64 =[リトルエンディアン下位ビットから]=> 1000 0110 0100 0110 1100 0110 0010 0110

### 計算の手順

```
 1:crc 初期値　　　    :00000000000000000000000000000000
 2:crc 初期値を反転    :11111111111111111111111111111111
 3:入力データ　        :10000110010001101100011000100110
 4:後続の0 を追加      :1000011001000110110001100010011000000000000000000000000000000000
 5:先頭8 bitをxor　　　:01111001 => 0x9e => 158
 6:配列の158 番目　    :--------11000010011111011110110111101000
 7:crc の残り24 bit とテーブルの値をxor
 8:新たなcrc の値　    :        00111101100000100001001011101000
 9:次の入力8 bit　　   :        01000110
10:次の8bit をxor　　　:        01111011 => 0xde => 222
11:配列の222 番目　    :----------------11001011111111111101011010000110
12:crc の残り24 bit とテーブルの値をxor
13:新たなcrc の値　    :                01001001111011010011111010000110
14:次の入力8 bit　　   :                11000110
15:次の8bit をxor　　　:                10001111 => 0xf1 => 241
16:配列の241 番目　    :------------------------01010001010000110101110101010011
17:crc の残り24 bit とテーブルの値をxor
18:新たなcrc の値　    :                        10111100011111011101101101010011
19:次の入力8 bit　　   :                        00100110
20:次の8bit をxor　　　:                        10011010 => 0x59 => 89
21:配列の89 番目　     :--------------------------------00001010100101111110110101001000
22:crc の残り24 bit とテーブルの値をxor
23:新たなcrc の値　    :                                01110111010011001011111001001000
24:ビットを反転        :                                10001000101100110100000110110111

1000 1000 1011 0011 0100 0001 1011 0111 => 0xed82cd11
```

* 最初の8 bit はmodulo 2 で消えるだけなのでビット反転だけ適用されている
* 次の8 bit は元の手順通りだと(入力 ^ 反転用0xff ^ 前8 bit のテーブルの値) だが6~8 行目で先に(反転用0xff ^ 前8 bit のテーブルの値) が計算され8~10 行目で(反転用0xff ^ 前8 bit のテーブルの値 ^ 入力) となっている
* 以降も8 bit ごとに３つの値のxor が順序を入れ替えて適用されている状態

### 操作を１つずつ分解したC++ コード

```cpp
constexpr uint32_t table[256] { /* 入力ごとの値を事前計算しておく */ };
uint32_t crc32(uint8_t * data, size_t size, uint32_t crc = 0) {
    // 初期値を反転させる
    crc ^= 0xffffffff;
    for (size_t i = 0; i < size; ++i) {
        // 次の入力8 bit
        auto input = data[i];
        // crc の先頭8 bit
        auto first_8bit = crc & 0xff;
        // 入力8 bit とcrc の先頭8 bit のxor が配列番号
        // 計算手順の5 行目
        auto index = input ^ first_8bit;
        // 事前計算の値
        auto pre_calc = table[index];
        // crc の後続24 bit
        auto next_24bit = crc >> 8;
        // 事前計算の値とcrc の後続24 bit をxor したものが新たなcrc の値
        // 計算手順の6~8 行目
        crc = pre_calc ^ next_24bit;
    }
    // 結果のビットを判定して返す
    return crc ^ 0xffffffff;
}
```

### 処理をまとめた実際のC++ コード

```cpp
constexpr uint32_t table[256] { /* 入力ごとの値を事前計算しておく */ };
uint32_t crc32(uint8_t * data, size_t size, uint32_t crc = 0) {
    crc ^= 0xffffffff;
    for (uint32_t i = ; i < size; ++i) {
        crc = table[(crc & 0xff) ^ data[i]] ^ (crc >> 8);
    }
    return crc ^ 0xffffffff;
}
```
